package cmd

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/spf13/cobra"

	agentconfig "github.com/akashicode/kash/internal/config"
	"github.com/akashicode/kash/internal/display"
)

var initCmd = &cobra.Command{
	Use:   "init <name>",
	Short: "Scaffold a new Kash agent project",
	Long: `Creates a new agent project directory with the required boilerplate:
  - data/          Empty directory for your PDF/Markdown source files
  - agent.yaml     Persona, models, and interface configuration
  - Dockerfile     Pre-configured for building Docker image
  - .env.example   Template for runtime environment variables
  - .dockerignore  Excludes raw data files from the final image

Also generates ~/.kash/config.yaml (if it doesn't already exist)
with an empty skeleton for LLM, embedder, and reranker settings.`,
	Args: cobra.ExactArgs(1),
	RunE: runInit,
}

func runInit(cmd *cobra.Command, args []string) error {
	name := args[0]
	projectDir := filepath.Join(".", name)

	// Check if directory already exists
	if _, err := os.Stat(projectDir); err == nil {
		return fmt.Errorf("directory %q already exists", name)
	}

	// Ensure ~/.kash/config.yaml exists
	created, err := agentconfig.EnsureConfigFile()
	if err != nil {
		return fmt.Errorf("create config file: %w", err)
	}

	display.Header("⚡ Initializing Agent Project: " + name)
	fmt.Println()

	// Create directory structure
	dirs := []string{
		projectDir,
		filepath.Join(projectDir, "data"),
	}
	for _, dir := range dirs {
		if err := os.MkdirAll(dir, 0755); err != nil {
			return fmt.Errorf("create directory %q: %w", dir, err)
		}
	}

	// Write agent.yaml
	agentYAML := generateAgentYAML(name)
	if err := writeFile(filepath.Join(projectDir, "agent.yaml"), agentYAML); err != nil {
		return fmt.Errorf("write agent.yaml: %w", err)
	}
	display.FileCreated("agent.yaml")

	// Write Dockerfile
	if err := writeFile(filepath.Join(projectDir, "Dockerfile"), generateDockerfile()); err != nil {
		return fmt.Errorf("write Dockerfile: %w", err)
	}
	display.FileCreated("Dockerfile")

	// Write .env.example
	if err := writeFile(filepath.Join(projectDir, ".env.example"), generateEnvExample()); err != nil {
		return fmt.Errorf("write .env.example: %w", err)
	}
	display.FileCreated(".env.example")

	// Write .dockerignore
	if err := writeFile(filepath.Join(projectDir, ".dockerignore"), generateDockerIgnore()); err != nil {
		return fmt.Errorf("write .dockerignore: %w", err)
	}
	display.FileCreated(".dockerignore")

	// Write README
	if err := writeFile(filepath.Join(projectDir, "README.md"), generateReadme(name)); err != nil {
		return fmt.Errorf("write README.md: %w", err)
	}
	display.FileCreated("README.md")

	// Write docker-compose.yml
	if err := writeFile(filepath.Join(projectDir, "docker-compose.yml"), generateDockerCompose(name)); err != nil {
		return fmt.Errorf("write docker-compose.yml: %w", err)
	}
	display.FileCreated("docker-compose.yml")

	fmt.Println()
	display.Success("Project created successfully!")
	fmt.Println()

	// Warn about empty config
	cfgPath, _ := agentconfig.ConfigFilePath()
	if created {
		display.Info(fmt.Sprintf("Config file created: %s", cfgPath))
		display.Warn("Please fill in your LLM and embedder API keys before running 'kash build'.")
		fmt.Println()
	} else if !agentconfig.IsConfigured() {
		display.Warn(fmt.Sprintf("Config file is empty: %s", cfgPath))
		display.Warn("Fill in your LLM and embedder API keys, or 'kash build' will fail.")
		fmt.Println()
	}

	display.NextSteps([]string{
		fmt.Sprintf("cd %s", name),
		fmt.Sprintf("Edit %s with your API keys", cfgPath),
		"Add documents to the data/ directory",
		"Edit agent.yaml to configure your agent's persona",
		"Run: kash build",
		"Copy .env.example to .env (for Docker runtime keys)",
		"Run: docker compose up --build",
	})

	return nil
}

func writeFile(path, content string) error {
	return os.WriteFile(path, []byte(content), 0644)
}

func generateAgentYAML(name string) string {
	slug := strings.ToLower(strings.ReplaceAll(name, " ", "-"))
	return fmt.Sprintf(`# Kash Agent Configuration
# Generated by: kash init %s

agent:
  name: "%s"
  version: "1.0.0"
  description: "An expert AI agent powered by Kash"

  # System prompt injected at runtime for all interfaces
  system_prompt: |
    You are %s, a highly knowledgeable expert assistant.
    You have access to a specialized knowledge base compiled from expert documents.
    Always ground your responses in the retrieved context.
    Be precise, helpful, and cite your knowledge when possible.

# Runtime settings
# Model names are resolved from env vars / config.yaml (not here).
# Only embedding dimensions need to be consistent between build and serve.
runtime:
  embedder:
    dimensions: 1024  # must match at build AND serve time
    # max_tokens: 8192  # optional: max token limit for the embedding model
                        # if set, chunk sizes are auto-tuned to stay within this limit
                        # check your model docs (e.g. voyage-3: 32000, text-embedding-3-small: 8191)
    # parallel: true    # optional: enable parallel embedding requests (for local embedders)
                        # default: false (sequential with retry, safe for hosted APIs)

# MCP tool definitions (auto-populated by 'kash build')
mcp:
  tools:
    - name: "search_%s_knowledge"
      description: "Search the expert knowledge base for relevant information. Use this tool when the user asks about topics covered in the agent's domain."
      # NOTE: This description will be auto-optimized by 'kash build'

# Server configuration
server:
  port: 8000
  cors_origins:
    - "*"
`, name, name, name, slug)
}

func generateDockerfile() string {
	return `# Kash Runtime Dockerfile
# Packages compiled databases with the kash binary (~50MB)
#
# Uses the multi-arch base image (amd64 + arm64) published to GHCR.
# At runtime it executes "kash serve" which starts the HTTP server
# exposing REST, MCP, and A2A interfaces on port 8000.
#
# Build for current architecture:
#   docker build -t my-agent:latest .
#
# Build multi-arch and push (share with the world):
#   docker buildx build --platform linux/amd64,linux/arm64 -t my-registry/my-agent:v1 --push .

FROM ghcr.io/akashicode/kash:latest

WORKDIR /app

# Copy the compiled database artifacts from 'kash build'
COPY data/memory.chromem/ /app/data/memory.chromem/
COPY data/knowledge.cayley/ /app/data/knowledge.cayley/

# Copy the agent configuration
COPY agent.yaml /app/agent.yaml

# Runtime API keys injected via environment variables (BYOM)
# Required:
ENV LLM_BASE_URL=""
ENV LLM_API_KEY=""
ENV LLM_MODEL=""
ENV EMBED_BASE_URL=""
ENV EMBED_API_KEY=""
ENV EMBED_MODEL=""
ENV EMBED_DIMENSIONS="1024"
# Optional reranker:
ENV RERANK_BASE_URL=""
ENV RERANK_API_KEY=""
ENV RERANK_MODEL=""

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=3s --start-period=5s \
  CMD wget -qO- http://localhost:8000/health || exit 1

ENTRYPOINT ["/app/kash", "serve"]
`
}

func generateDockerCompose(name string) string {
	slug := strings.ToLower(strings.ReplaceAll(name, " ", "-"))
	return fmt.Sprintf(`# docker-compose.yml
# Builds and runs the %s agent locally.
#
# Usage:
#   1. Copy .env.example to .env and fill in your API keys
#   2. Run: kash build
#   3. Run: docker compose up --build

services:
  agent:
    build: .
    image: %s:latest
    ports:
      - "8000:8000"
    env_file:
      - .env
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    restart: unless-stopped
`, name, slug)
}

func generateEnvExample() string {
	return `# Kash Runtime Environment Variables
# Copy this to .env and fill in your values
# Then: docker run --env-file .env my-agent:latest

# LLM Provider (required) - must be OpenAI-compatible
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=sk-your-key-here
LLM_MODEL=gpt-4o

# Embedding Provider (required) - must be OpenAI-compatible
# Model is optional when using an embedding router.
EMBED_BASE_URL=https://api.voyageai.com/v1
EMBED_API_KEY=pa-your-key-here
EMBED_MODEL=

# Reranking Provider (optional) - must be OpenAI-compatible
# RERANK_BASE_URL=
# RERANK_API_KEY=
# RERANK_MODEL=

# API Key (optional) - when set, all endpoints (except /health) require auth.
# Works as Bearer token with curl, as api_key with OpenAI SDK,
# and as API_KEY env var for MCP clients.
# AGENT_API_KEY=my-secret-api-key
`
}

func generateDockerIgnore() string {
	return `# Exclude raw source files from Docker image
# The compiled databases (memory.chromem/, knowledge.cayley/) are what matters

# Raw data sources
data/*.pdf
data/*.md
data/*.txt
data/*.docx

# Development artifacts
*.log
*.tmp
.env
.env.local

# Go build artifacts (if any)
bin/
`
}

func generateReadme(name string) string {
	return fmt.Sprintf(`# %s — Kash Agent

An expert AI agent built with [Kash](https://github.com/akashicode/kash).

## Quick Start

### Prerequisites
- [Kash CLI](https://github.com/akashicode/kash) installed
- OpenAI-compatible LLM and embedding API access
- Docker (with buildx for multi-arch)

### Build

1. Add your knowledge documents to the `+"`data/`"+` directory:
   `+"```"+`bash
   cp my-document.pdf data/
   cp my-notes.md data/
   `+"```"+`

2. Configure your build providers in `+"`~/.kash/config.yaml`"+`

3. Compile the knowledge base:
   `+"```"+`bash
   kash build
   `+"```"+`

4. Build the Docker image (current architecture):
   `+"```"+`bash
   docker build -t %s:latest .
   `+"```"+`

5. Or build multi-arch and push (share with the world):
   `+"```"+`bash
   docker buildx build --platform linux/amd64,linux/arm64 -t my-registry/%s:v1 --push .
   `+"```"+`

### Run

**Easy way — Docker Compose:**

`+"```"+`bash
cp .env.example .env   # fill in your API keys
docker compose up --build
`+"```"+`

**Manual — docker run:**

`+"```"+`bash
docker run -p 8000:8000 \\
  -e LLM_BASE_URL="https://api.openai.com/v1" \\
  -e LLM_API_KEY="sk-..." \\
  -e LLM_MODEL="gpt-4o" \\
  -e EMBED_BASE_URL="https://api.voyageai.com/v1" \\
  -e EMBED_API_KEY="pa-..." \\
  -e EMBED_MODEL="voyage-3" \\
  -e RERANK_BASE_URL=""   `+"`# optional`"+` \\
  -e RERANK_API_KEY=""    `+"`# optional`"+` \\
  -e RERANK_MODEL=""      `+"`# optional`"+` \\
  %s:latest
`+"```"+`

## Interfaces

- **REST API**: `+"`POST http://localhost:8000/v1/chat/completions`"+` (OpenAI-compatible)
- **MCP Server**: `+"`GET http://localhost:8000/mcp`"+` (for Cursor, Windsurf)
- **A2A Protocol**: `+"`POST http://localhost:8000/rpc/agent`"+` (for AutoGen, CrewAI)
`, name, name, name, name)
}
